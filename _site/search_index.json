[["introduction.html", "1 Introduction", " 1 Introduction Hi, welcome to My Portfolio! My name is Rachelle Balgit, and this portfolio serves as a comprehensive collection of my work and projects in the fields of data analysis and scientific research. As a student in life sciences, I have dedicated myself to exploring and analyzing biological data to uncover insights that are both impact- and meaningful. This portfolio showcases a diverse array of projects and assignments, ranging from academic research endeavors to practical data analysis and visualization tasks. Each project is crafted to highlight my proficiency in data analysis, programming, and problem-solving, demonstrating the skills and expertise I have developed throughout this course. "],["cv.html", "2 CV 2.1 Education 2.2 Important Courses 2.3 Projects 2.4 Minor Data science for Biology 2.5 Important Skills", " 2 CV Motivated student persuing degree in biomedical sciences, with a big interest in microbiology and data science. Excellent analytical and communication skills. Speaks fluent Dutch and English. Always eager to learn and seeking for new experiences. I am always hard working, organized and capable of working in a team or individual. 2.1 Education Bachelor’s Degree Life Sciences (biomedical sciences), Hogeschool Utrecht (2020-current) • Specializing in microbiology • Minor in Data science for biology part 1 and 2 2.2 Important Courses • Medical Microbiological Diagnosis • Laboratory Tools • Research in Microbiology • Immunology • Experimental Design 2.3 Projects Project Genes &amp; Proteins: Determination of expression of IL-8 with a green fluorescent protein Researching increased IL-8 protein levels of patients with an RSV infection Project Microbiology: Determination of an infection associated with Neisseria gonorrhoeae of patients Using PCR and VITEK Project Research Microbiology: Researching the outer membrane vesicles of Porphyromonas gingivalis of patients with parodontitis Using spectrometry and different isolation methods 2.4 Minor Data science for Biology Data analysis using R RNA-sequencing Metagenomics Next Generation Sequencing 2.5 Important Skills • MALDI-TOF • VITEK • (Q)PCR • ELISA • Chromotography • Cultering of bacteria "],["maldiquant.html", "3 MALDIquant 3.1 Intro 3.2 Manual 3.3 References", " 3 MALDIquant 3.1 Intro Due to my interest in bacteria and viruses, I chose the specialization in microbiology in my third year. For a project, I worked with the bacterium Porphyromonas gingivalis, exposing it to different concentrations of magnesium and iron. To measure the effect of these elements on the number of secreted outer membrane vesicles (OMVs), a fluorescence microscope and MALDI-TOF MS were used. For MALDI-TOF MS data, there is a package called MALDIquant, which provides a complete pipeline for analysis. Since I find MALDI-TOF MS to be an interesting technique in microbiology, I would like to explore how this package works. By finding information about the package online, I plan to apply it to my previous project to compare the results I had then with the ones I have now. I could start working on this in four days. 3.1.1 Schedule In the first two days, I will mainly practice using the package. There are various manuals available online that I can go through with fake data. Afterward, I can try running the package on my own data. Day one: explore MALDIquant (&amp; some MALDITOF-MS theory) Day two &amp; three: try some codes, manuals of MALDIquant Day four: Try the code on own data 3.1.2 Videos There is one video I found on YouTube, in which a girl explains her way of using MALDIquant: https://www.youtube.com/watch?v=KwCRV885Z-k&amp;t=214s 3.2 Manual The script shown below, uses the MALDIquant package to process and analyze MALDI-TOF MS data. It begins by installing and loading the necessary packages. A mass spectrum object is created to represent the data, followed by importing example data, specifically a subset of MALDI-TOF data from a study by Fiedler et al. (2009). Quality control checks are performed to ensure that all spectra have a consistent number of non-empty data points and that the data points are regular. The script then visualizes the spectra by plotting them, which provides an initial look at the data. Next, the script applies variance stabilization to the spectra, which is crucial for making the data more consistent. Smoothing is applied to reduce noise, followed by baseline correction to remove any background noise that might affect the analysis. After baseline correction, the spectra are normalized to ensure they are on a comparable scale. Spectra alignment is then performed, which involves adjusting the spectra so that peaks align correctly across samples. This step is essential for accurate comparison. The script identifies the samples within the spectra and averages them to create a single spectrum per sample. Peak detection is carried out on the averaged spectra, and noise estimation is included to identify and highlight significant peaks. These peaks are then binned, which groups similar peaks together based on a specified tolerance. Finally, the script creates a feature matrix, which contains the intensity values of these peaks, ready for further analysis 1. # Install the necessary packages # install.packages(c(&quot;MALDIquant&quot;, &quot;MALDIquantForeign&quot;)) library(MALDIquant) ## ## This is MALDIquant version 1.22.2 ## Quantitative Analysis of Mass Spectrometry Data ## See &#39;?MALDIquant&#39; for more information about this package. # Creating a MALDIquant mass spectrum object # mass is the m/z value # intensity is how many detected ions per peak s &lt;- createMassSpectrum(mass=1:10, intensity=1:10, metaData=list(name=&quot;Spectrum1&quot;)) s ## S4 class type : MassSpectrum ## Number of m/z values : 10 ## Range of m/z values : 1 - 10 ## Range of intensity values: 1 - 10 ## Memory usage : 1.414 KiB ## Name : Spectrum1 # Importing example data # A subset of MALDI-TOF data from Fiedler et al. (2009) data(&quot;fiedler2009subset&quot;) length(fiedler2009subset) ## [1] 16 fiedler2009subset[1:2] ## $sPankreas_HB_L_061019_G10.M19.T_0209513_0020740_18 ## S4 class type : MassSpectrum ## Number of m/z values : 42388 ## Range of m/z values : 1000.015 - 9999.734 ## Range of intensity values: 5 - 101840 ## Memory usage : 506.359 KiB ## Name : Pankreas_HB_L_061019_G10.M19 ## File : /data/set A - discovery leipzig/control/Pankreas_HB_L_061019_G10/0_m19/1/1SLin/fid ## ## $sPankreas_HB_L_061019_G10.M20.T_0209513_0020740_18 ## S4 class type : MassSpectrum ## Number of m/z values : 42388 ## Range of m/z values : 1000.015 - 9999.734 ## Range of intensity values: 6 - 111862 ## Memory usage : 506.359 KiB ## Name : Pankreas_HB_L_061019_G10.M20 ## File : /data/set A - discovery leipzig/control/Pankreas_HB_L_061019_G10/0_m20/1/1SLin/fid # Quality control: Check if all spectra have the same number of non-empty data points any(sapply(fiedler2009subset, isEmpty)) ## [1] FALSE table(sapply(fiedler2009subset, length)) ## ## 42388 ## 16 all(sapply(fiedler2009subset, isRegular)) ## [1] TRUE # Plotting the spectra plot(fiedler2009subset[[1]]) plot(fiedler2009subset[[16]]) fiedler2009subset[16] ## $sPankreas_HB_L_061019_D9.G18.T_0209513_0020740_18 ## S4 class type : MassSpectrum ## Number of m/z values : 42388 ## Range of m/z values : 1000.015 - 9999.734 ## Range of intensity values: 7 - 22786 ## Memory usage : 506.367 KiB ## Name : Pankreas_HB_L_061019_D9.G18 ## File : /data/set B - discovery heidelberg/tumor/Pankreas_HB_L_061019_D9/0_g18/1/1SLin/fid # Variance stabilization spectra &lt;- transformIntensity(fiedler2009subset, method = &quot;sqrt&quot;) # Smoothing the spectra spectra &lt;- smoothIntensity(spectra, method=&quot;SavitzkyGolay&quot;, halfWindowSize=10) # Baseline correction baseline &lt;- estimateBaseline(spectra[[16]], method=&quot;SNIP&quot;, iterations=100) plot(spectra[[16]]) lines(baseline, col=&quot;red&quot;, lwd=2) spectra &lt;- removeBaseline(spectra, method=&quot;SNIP&quot;, iterations=100) plot(spectra[[1]]) # Normalization spectra &lt;- calibrateIntensity(spectra, method=&quot;TIC&quot;) # Alignment of spectra spectra &lt;- alignSpectra(spectra, halfWindowSize=20, SNR=2, tolerance=0.002, warpingMethod=&quot;lowess&quot;) samples &lt;- factor(sapply(spectra, function(x)metaData(x)$sampleName)) avgSpectra &lt;- averageMassSpectra(spectra, labels=samples, method=&quot;mean&quot;) # Peak detection noise &lt;- estimateNoise(avgSpectra[[1]]) plot(avgSpectra[[1]], xlim=c(4000, 5000), ylim=c(0, 0.002)) lines(noise, col=&quot;red&quot;) lines(noise[,1], noise[, 2]*2, col=&quot;blue&quot;) peaks &lt;- detectPeaks(avgSpectra, method=&quot;MAD&quot;, halfWindowSize=20, SNR=2) plot(avgSpectra[[1]], xlim=c(4000, 5000), ylim=c(0, 0.002)) points(peaks[[1]], col=&quot;red&quot;, pch=4) # Peak binning peaks &lt;- binPeaks(peaks, tolerance=0.002) # Creating a feature matrix peaks &lt;- filterPeaks(peaks, minFrequency=0.25) featureMatrix &lt;- intensityMatrix(peaks, avgSpectra) head(featureMatrix[, 1:3]) ## 1011.73182227583 1020.6748082171 1029.40115131151 ## [1,] 0.0001894947 0.0007715987 0.0001093035 ## [2,] 0.0002144354 0.0015030560 0.0001422394 ## [3,] 0.0002117147 0.0004555688 0.0001303326 ## [4,] 0.0002314181 0.0005260977 0.0001441254 ## [5,] 0.0001562401 0.0024054031 0.0001198008 ## [6,] 0.0001600630 0.0020315191 0.0001090484 # Plot example results par(mfrow = c(2, 3)) xlim &lt;- range(mass(spectra[[1]])) # Use the same xlim for all plots for better comparison plot(spectra[[1]], main = &quot;1: raw&quot;, sub = &quot;&quot;, xlim = xlim) plot(spectra[[1]], main = &quot;2: variance stabilization&quot;, sub = &quot;&quot;, xlim = xlim) plot(spectra[[1]], main = &quot;3: smoothing&quot;, sub = &quot;&quot;, xlim = xlim) plot(spectra[[1]], main = &quot;4: baseline correction&quot;, sub = &quot;&quot;, xlim = xlim) plot(spectra[[1]], main = &quot;5: peak detection&quot;, sub = &quot;&quot;, xlim = xlim) points(peaks[[1]]) par(mfrow = c(1, 1)) Creating a MALDIquant mass spectrum object -&gt; you would only use this function when you are working with raw/custom data that you need to put manually in Rstudio. The fiedler2009subdata is already in the MassSpectrum object. Without your data in a MassSpectrum object you would need to manually structure your data for every step of the analysis, which would be prone to errors. 3.3 References Deepika, D., &amp; Kumar, V. (2023). The role of “Physiologically Based Pharmacokinetic Model (PBPK)” New Approach Methodology (NAM) in Pharmaceuticals and Environmental Chemical Risk assessment. International Journal of Environmental Research and Public Health, 20(4), 3473. https://doi.org/10.3390/ijerph20043473↩︎ "],["open-peer-review.html", "4 Open Peer Review 4.1 Article 4.2 Summary 4.3 Review", " 4 Open Peer Review 4.1 Article An, A.Y. et al. (2023) Dynamic gene expression analysis reveals distinct severity phases of immune and cellular dysregulation in COVID-19, bioRxiv. Available at: https://www.biorxiv.org/content/10.1101/2023.11.04.565404v1.full (Accessed: 02 May 2024). 4.2 Summary Objective To investigate longitudinal changes in gene expression profiles throughout the COVID-19 disease timeline. Methods Three-hundred whole blood samples from 128 adult patients were collected during hospitalization from COVID-19, with up to five samples per patient. Transcriptome sequencing (RNA-Seq), differential gene expression analysis and pathway enrichment was performed. Drug-gene set enrichment analysis was used to identify FDA-approved medications that could inhibit critical genes and proteins at each disease phase. Prognostic gene-expression signatures were generated using machine learning to distinguish 3 disease stages. Results Samples were longitudinally grouped by clinical criteria and gene expression into six disease phases: Mild, Moderate, Severe, Critical, Recovery, and Discharge. Distinct mechanisms with differing trajectories during COVID-19 hospitalization were apparent. Antiviral responses peaked early in COVID-19, while heme metabolism pathways became active much later during disease. Adaptive immune dysfunction, inflammation, and metabolic derangements were most pronounced during phases with higher disease severity, while hemostatic abnormalities were elevated early and persisted throughout the disease course. Drug-gene set enrichment analysis predicted repurposed medications for potential use, including platelet inhibitors in early disease, antidiabetic medications for patients with increased disease severity, and dasatinib throughout the disease course. Disease phases could be categorized using specific gene signatures for prognosis and treatment selection. Disease phases were also highly correlated to previously developed sepsis endotypes, indicating that severity and disease timing were significant contributors to heterogeneity observed in sepsis and COVID-19. Conclusions Higher temporal resolution of longitudinal mechanisms in COVID-19 revealed multiple immune and cellular changes that were activated at different phases of COVID-19. Understanding how a patient’s gene expression profile changes over time can permit more accurate risk stratification of patients and provide time-dependent personalized treatments with repurposed medications. This creates an opportunity for timely intervention before patients transition to a more severe phase, potentially accelerating patients to recovery. 4.3 Review The article has been evaluated according to the criteria listed in the table below. Transparency Criteria Definition Response Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. yes Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. yes Data Location Where the article’s data can be accessed, either raw or processed. The datasets generated for this study can be found on GEO: GSE221234 and GSE222253. Code is available upon request (Dr. Robert Hancock, bob{at}hancocklab.com). Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. yes, Author Review The professionalism of the contact information that the author has provided in the manuscript. yes, with information of the researchers and an e-mail. Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. yes Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. yes Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. no "],["sql.html", "5 SQL", " 5 SQL # Load required libraries library(tidyr) library(dplyr) library(stringr) library(DBI) library(RPostgres) library(readr) library(dslabs) library(ggplot2) # Load in data flu_data &lt;- read_csv(&quot;~/Rachelle/flu_data.csv&quot;, skip = 11) dengue_data &lt;- read_csv(&quot;~/Rachelle/dengue_data.csv&quot;, skip = 11) gapminder_data &lt;- gapminder # Make data tidy flu_tidy &lt;- pivot_longer(data = flu_data, cols = c(2:30), names_to = &quot;Country&quot;, values_to = &quot;Value&quot;) flu_tidy &lt;- flu_tidy %&gt;% mutate(year = str_sub(Date, 1, 4)) %&gt;% select(-Date) dengue_tidy &lt;- pivot_longer(data = dengue_data, cols = c(2:11), names_to = &quot;Country&quot;, values_to = &quot;Value&quot;) dengue_tidy &lt;- dengue_tidy %&gt;% mutate(year = str_sub(Date, 1, 4)) %&gt;% select(-Date) gapminder_data &lt;- as_tibble(gapminder_data) gapminder_data &lt;- gapminder_data %&gt;% rename(Year = year, Country = country) %&gt;% mutate(Year = as.character(Year)) # Convert Year to character # Save dataframes write.csv(flu_tidy, &quot;flu_tidy.csv&quot;) write.csv(dengue_tidy, &quot;dengue_tidy.csv&quot;) write.csv(gapminder_data, &quot;gapminder_tidy.csv&quot;) saveRDS(flu_tidy, &quot;flu_tidy.rds&quot;) saveRDS(dengue_tidy, &quot;dengue_tidy.rds&quot;) saveRDS(gapminder_data, &quot;gapminder_tidy.rds&quot;) # Connect to database in PostgreSQL con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;password&quot;) dbWriteTable(con, &quot;gapminder&quot;, gapminder_data) dbWriteTable(con, &quot;flu&quot;, flu_tidy) dbWriteTable(con, &quot;dengue&quot;, dengue_tidy) # Inspect contents of the tables knitr::include_graphics(&quot;C:\\\\Users\\\\rache\\\\OneDrive\\\\Documenten\\\\Schermafbeelding 2024-08-18 230338.png&quot;) knitr::include_graphics(&quot;C:\\\\Users\\\\rache\\\\OneDrive\\\\Documenten\\\\Schermafbeelding 2024-08-18 230429.png&quot;) knitr::include_graphics(&quot;C:\\\\Users\\\\rache\\\\OneDrive\\\\Documenten\\\\Schermafbeelding 2024-08-18 230509.png&quot;) # Join the tables together flu_tidy &lt;- flu_tidy %&gt;% mutate(year = as.character(year)) dengue_tidy &lt;- dengue_tidy %&gt;% mutate(year = as.character(year)) flu_dengue &lt;- full_join(flu_tidy, dengue_tidy, by = c(&quot;Country&quot;, &quot;year&quot;), suffix = c(&quot;_flu&quot;, &quot;_dengue&quot;)) # Rename &#39;year&#39; to &#39;Year&#39; in flu_dengue for consistency with gapminder_data flu_dengue &lt;- flu_dengue %&gt;% rename(Year = year) # Join with gapminder flu_dengue_gapminder &lt;- inner_join(flu_dengue, gapminder_data, by = c(&quot;Country&quot;, &quot;Year&quot;)) # Check the resulting dataframe print(flu_dengue_gapminder) ## # A tibble: 155,161 × 11 ## Country Value_flu Year Value_dengue infant_mortality life_expectancy ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argentina NA 2002 NA 17.1 74.3 ## 2 Australia NA 2002 NA 5 80.3 ## 3 Austria NA 2002 NA 4.4 78.8 ## 4 Belgium NA 2002 NA 4.4 78.2 ## 5 Bolivia NA 2002 0.101 53.7 68.7 ## 6 Brazil 174 2002 0.073 24.3 71.4 ## 7 Bulgaria NA 2002 NA 16.3 72.1 ## 8 Canada NA 2002 NA 5.3 79.6 ## 9 Chile NA 2002 NA 8.3 77.7 ## 10 France NA 2002 NA 4.2 79.4 ## # ℹ 155,151 more rows ## # ℹ 5 more variables: fertility &lt;dbl&gt;, population &lt;dbl&gt;, gdp &lt;dbl&gt;, ## # continent &lt;fct&gt;, region &lt;fct&gt; # Boxplot dengue cases per year in Brazil dengue_Brazil &lt;- flu_dengue_gapminder %&gt;% select(Year, Country, Value_dengue) %&gt;% filter(Country == &quot;Brazil&quot;, !is.na(Value_dengue)) dengue_sum &lt;- dengue_Brazil %&gt;% group_by(Year) %&gt;% summarise(mean = mean(Value_dengue, na.rm = TRUE), stedv = sd(Value_dengue, na.rm= TRUE)) # Barchart of dengue cases per year dengue_plot &lt;- ggplot(dengue_sum, aes(x = as.factor(Year), y = mean)) + geom_col(fill = &quot;lightpink&quot;, color = &quot;grey&quot;) + labs(title = &quot;Dengue cases Brazil (in years)&quot;, x = &quot;Year&quot;, y = &quot;Average cases of dengue&quot;) + theme_minimal() dengue_plot "],["datetime-r-package.html", "6 DateTime R Package 6.1 Introduction 6.2 Installation 6.3 Fuctions of the package", " 6 DateTime R Package 6.1 Introduction This is a user guide for the DateTime package, which allows you to perform operations or calculations with date and time. See this guide for explanations and examples of the functions in this package. 6.2 Installation To use the DateTime package, use the code below in your console. # install packages &quot;usethis&quot; and &quot;devtools&quot; if they are not already installed library(usethis) library(devtools) # Load the DateTime package in library(DateTime) 6.3 Fuctions of the package The package contains 4 functions which are related to date and time. date_difference() add_time() is_same_day() is_dutch_holiday() 6.3.1 Calculating date difference The date_difference() function allows you to calculate the difference between two dates. The function uses various time units such as seconds, minutes, hours, days, weeks, months, and years. # Load the DateTime package library(DateTime) # Calculate the difference between two dates in days date_difference(&quot;2024-05-21&quot;, &quot;2024-05-22&quot;, units = &quot;days&quot;) ## Time difference of 1 days # Calculate the difference between two dates in weeks date_difference(&quot;2024-05-21&quot;, &quot;2024-06-21&quot;, units = &quot;weeks&quot;) ## Time difference of 4.428571 weeks # Calculate the difference between two dates in years date_difference(&quot;2024-05-21&quot;, &quot;2025-05-22&quot;, units = &quot;years&quot;) ## [1] &quot;Time difference of 1.00205338809035 years&quot; 6.3.2 Add time to a date The add_time() function adds a certain amount of time to the filled in date. # Add 1 hour add_time(&quot;2024-05-21 08:00:00&quot;, 1, &quot;hours&quot;) ## [1] &quot;2024-05-21 09:00:00 CEST&quot; # Add 2 weeks add_time(&quot;2024-07-22 07:00:00&quot;, 2, &quot;weeks&quot;) ## [1] &quot;2024-08-05 07:00:00 CEST&quot; # Add 54 seconds add_time(&quot;2024-05-21 08:00:00&quot;, 54, &quot;seconds&quot;) ## [1] &quot;2024-05-21 08:00:54 CEST&quot; 6.3.3 Check if two dates are on the same day The is_same_day() function checks whether two dates fall on the same weekday. If so, it returns “TRUE” and gives the name of the weekday. If not, it returns “FALSE” and gives you the two different weekday names. # Check if the 21st of May 2024 falls on the same day as 21st of May 2024. is_same_day(&quot;2024-05-21&quot;, &quot;2024-05-21&quot;) ## $same_day ## [1] TRUE ## ## $day_of_week ## [1] &quot;dinsdag&quot; # Check if the 4th of May 2024 falls on the same day as the 2nd of February 2024 is_same_day(&quot;2024-05-04&quot;, &quot;2024-02-02&quot;) ## $same_day ## [1] FALSE ## ## $day_of_week_1 ## [1] &quot;zaterdag&quot; ## ## $day_of_week_2 ## [1] &quot;vrijdag&quot; 6.3.4 Check for Dutch holidays The is_dutch_holiday() function checks if the given date, is also a (legal) Dutch holiday. These holidays are: New Year’s Day Goede Vrijdag / Good Friday Easter Second Easter Day King’s Day Liberation Day (every 5 years) Ascension Day Pentecost Second Pentecost Christmas Day Second Christmas Day is_dutch_holiday(&quot;2024-01-01&quot;) ## [1] TRUE is_dutch_holiday(&quot;2024-08-12&quot;) ## [1] FALSE "],["projecticum-ner.html", "7 Projecticum NER 7.1 Introduction 7.2 Physiologically Based Pharmacokinetic models 7.3 Prodigy 7.4 Named Entity Recognition models 7.5 References", " 7 Projecticum NER 7.1 Introduction For the Data science For Biology course I’m working on a project to help develop Physiologically Based Kinetics (PBK) models, which are used to simulate how chemicals are metabolized in the human body. Building these models is a challenging process that requires collecting a lot of data, like chemical concentrations, tissue properties, and toxicokinetic profiles. This can be time-consuming, error-prone, and difficult to reproduce. My role in the project is to train a small Named Entity Recognition (NER) model that can automatically identify and extract specific details from the methods sections of scientific articles, such as cell culture conditions and incubation times. I’ll be training the model with the use of Python, and my first step is to get familiar with the annotation tool Prodigy and set up a prototype of the model. 7.2 Physiologically Based Pharmacokinetic models Physiologically Based Pharmacokinetic (PBPK) models are mathematical models encompassing multiple compartments with physiology, anatomy, biochemical and physicochemical parameters for describing ADME (absorption, distribution, metabolism and excretion) of xenobiotics and their metabolites. These models vary from empirical, semi-mechanistic to compartmental models based on the complexity of the problem. The major challenge with empirical or semi-mechanistic models is their difficulty with interpreting questions such as “How to predict concentration–time profile of compound in target organ” or “How to accurately predict the exact dosing while extrapolating from animal to human”. This led to the development of compartmental models, which are close to human anatomy and physiology. Currently, these models are widely acknowledged in the field of pharmaceutical and environmental science for the prediction of PK behavior of xenobiotics (drug/chemical) with respect to dose, route and species 2. 7.3 Prodigy Machine learning systems are built from both code and data. It’s easy to reuse the code but hard to reuse the data, so building AI mostly means doing annotation. Prodigy addresses the big remaining problem: annotation and training. The typical approach to annotation forces projects into an uncomfortable waterfall process. The experiments can’t begin until the first batch of annotations are complete, but the annotation team can’t start until they receive the annotation manuals. To produce the annotation manuals, you need to know what statistical models will be required for the features you’re trying to build. Machine learning is an inherently uncertain technology, but the waterfall annotation process relies on accurate upfront planning. The net result is a lot of wasted effort. Prodigy solves this problem by letting data scientists conduct their own annotations, for rapid prototyping Most annotation tools avoid making any suggestions to the user, to avoid biasing the annotations. Prodigy takes the opposite approach: ask the user as little as possible, and try to guess the rest. Prodigy puts the model in the loop, so that it can actively participate in the training process and learns as you go. The model uses what it already knows to figure out what to ask you next. As you answer the questions, the model is updated, influencing which examples it asks you about next. In order to take full advantage of this strategy, Prodigy is provided as a Python library and command line utility, with a flexible web application 3. 7.4 Named Entity Recognition models Named Entity Recognition (NER) is a critical component of Natural Language Processing (NLP) that involves identifying and classifying named entities in text into predefined categories such as people, organizations, locations, dates, and more. spaCy, a robust NLP library in Python, offers advanced tools for NER, providing a user-friendly API and powerful models. Named Entity Recognition is a sub-task of information extraction that aims to locate and classify entities within text. These entities are categorized into various predefined classes such as: NER is pivotal for tasks like information retrieval, question answering, and text summarization. It helps in structuring unstructured data, making it easier to analyze and use in applications. spaCy is an open-source NLP library designed for efficient and productive development. It supports tokenization, part-of-speech tagging, dependency parsing, and NER. spaCy’s models are pre-trained on large datasets, which makes it an excellent choice for building robust NLP applications 4. 7.5 References Deepika, D., &amp; Kumar, V. (2023). The role of “Physiologically Based Pharmacokinetic Model (PBPK)” New Approach Methodology (NAM) in Pharmaceuticals and Environmental Chemical Risk assessment. International Journal of Environmental Research and Public Health, 20(4), 3473. https://doi.org/10.3390/ijerph20043473↩︎ Montani, I. (2019, August 22). Getting started with Prodigy: A step-by-step guide. Ines Montani. https://ines.io/blog/prodigy-first-steps↩︎ Mljourney. (2024, July 27). Named Entity Recognition with spaCy - ML Journey. ML Journey. https://mljourney.com/named-entity-recognition-with-spacy/↩︎ "],["parameterized-covid-19-report.html", "8 Parameterized COVID-19 report Importeren en inspecteren Line chart the Netherlands, Belgium and Germany Line chart Romania, Liechtenstein and Italy", " 8 Parameterized COVID-19 report Parameterized reporting is a technique that allows you to generate multiple reports simultaneously. By using parameterized reporting, you can follow the same process to make 3,000 reports as you would to make one report. The technique also makes your work more accurate, as it avoids copy-and-paste errors. In this chapter we’re making a parameterized report of COVID-19 data obtained from the European Center for Disease Control (ECDC). Importeren en inspecteren Line chart the Netherlands, Belgium and Germany Using the ECDC COVID-19 data, there is a visual representation of COVID-19 cases and deaths in selected countries and years. The graphs display the number of cases and deaths per date. By utilizing parameterization in the R code with params$, this report can be easily tailored to specific interests without needing to modify the underlying code. Figure 8.1: Line chart of the COVID-19 data from the Netherlands, Belgium and Germany, with the date in years and month on t x-axis, and the amount of cases on the y-axis. de datum in jaar en maand op de x-as staat, de aantal cases op de y-as. Figure 8.2: Line chart of the COVID-19 data from the Netherlands, Belgium and Germany, with the date in years and month on t x-axis, and the amount of cases on the y-axis. de datum in jaar en maand op de x-as staat, de aantal cases op de y-as. Line chart Romania, Liechtenstein and Italy To verify that the parameterization is working correctly, you can adjust the values in the parameter list. In this example, the countries Romania, Norway, and Italy are used, along with the year 2022 Figure 8.3: Line chart of COVID-19 data, with the date in year and month on the x-axis, the number of cases on the y-axis. Different colors represent the selected countries Romania, Liechtenstein, and Italy. Figure 8.4: Line chart of COVID-19 data, with the date in year and month on the x-axis, the number of cases on the y-axis. Different colors represent the selected countries Romania, Liechtenstein, and Italy. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
